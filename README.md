# Awesome SMLs

This is the list of the SMLs I use on my Raspberry Pi5 (8GB RAM) with [Ollama](https://ollama.com/):

| Name | Size | tag | Remark | kind | URL | Good on Pi5 | Usable on Pi5 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| CodeGemma 2b | 1.6GB | 2B | Fill-in-the-middle code completion | code | https://ollama.com/library/codegemma:2b |   | x |
| Gemma 2b | 1.7GB | 2B |   |   | https://ollama.com/library/gemma:2b |   | x |
| Gemma2 2b | 1.6GB | 2B |   |   | https://ollama.com/library/gemma2:2b |   | x |
| All-Minilm 22m | 46MB | 22M | Only Embeddings | embedding | https://ollama.com/library/all-minilm:22m | x | x |
| All-Minilm 33m | 67MB | 33M | Only Embeddings | embedding | https://ollama.com/library/all-minilm:33m | x | x |
| DeepSeek Coder 1.3b | 776MB | 1.3B | Trained on both 87% code and 13% natural language | code | https://ollama.com/library/deepseek-coder | x | x |
| TinyLlama 1.1b | 638MB | 1.1B |   |   | https://ollama.com/library/tinyllama | x | x |
| TinyDolphin 1.1b | 637MB | 1.1B |   |   | https://ollama.com/library/tinydolphin | x | x |
| Phi3 Mini | 2.4GB | 3B |   |   | https://ollama.com/library/phi3:mini |   | x |
| Phi3.5 | 2.2GB | 3B |   |   | https://ollama.com/library/phi3.5 |   | x |
| Granite-code 3b | 2.0GB | 3B |   | code | https://ollama.com/library/granite-code |   | x |
| Qwen2 0.5b | 352MB | 0.5B |   |   | https://ollama.com/library/qwen2:0.5b | x | x |
| Qwen2 1.5b | 934MB | 1.5B |   |   | https://ollama.com/library/qwen2:1.5b |   | x |
| Qwen 0.5b | 395MB | 0.5B |   |   | https://ollama.com/library/qwen:0.5b | x | x |
| Qwen2 Math 1.5b | 935MB | 1.5B | Specialized math language model | math | https://ollama.com/library/qwen2-math:1.5b |   | x |
| StarCoder 1b | 726MB | 1B | Code generation model | code | https://ollama.com/library/starcoder:1b | x | x |
| StarCoder2 3b | 1.7GB | 3B |   | code | https://ollama.com/library/starcoder2:3b |   | x |
| Stable LM 2 1.6b | 983MB | 1.6B | LLM trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch. |   | https://ollama.com/library/stablelm2 | x | x |
| Stable Code 3b | 1.6GB | 3B | Coding model | code | https://ollama.com/library/stable-code:3b |   | x |
| Replete-Coder Qwen2 1.5b | 1.9GB | 1.5B | Coding capabilities + non-coding data, fully cleaned and uncensored (mat+tool? to be tested) | code | https://ollama.com/rouge/replete-coder-qwen2-1.5b:Q8 | x | x |
| Dolphin-Phi 2.7b | 1.6GB | 2.7B | uncensored |   | https://ollama.com/library/dolphin-phi:2.7b |   | x |
| Dolphin gemma2 2b | 1.6GB | 2B |   |   | https://ollama.com/CognitiveComputations/dolphin-gemma2:2b |   | x |
| allenporter/xlam:1b | 873MB | 1B |   | tools | https://ollama.com/allenporter/xlam:1b |   | x |
| sam4096/qwen2tools:0.5b | 352MB | 0.5B |   | tools | https://ollama.com/sam4096/qwen2tools:0.5b | x | x |
| sam4096/qwen2tools:1.5b | 935MB | 1.5B |   | tools | https://ollama.com/sam4096/qwen2tools:1.5b |   | x |
| mxbai-embed-large | 670MB | 335M | Only Embeddings | embedding | https://ollama.com/library/mxbai-embed-large:335m | x | x |
| nomic-embed-text | 274MB | 137M | Only Embeddings | embedding | https://ollama.com/library/nomic-embed-text:v1.5 | x | x |
| Yi Coder 1.5b | 866MB | 1.5B | Code | code | https://ollama.com/library/yi-coder:1.5b |   | x |